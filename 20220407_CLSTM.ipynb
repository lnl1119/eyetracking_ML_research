{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convolutional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        " \n",
        "class ConvLSTMCell(nn.Module):\n",
        " \n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
        "        \"\"\"\n",
        "        Initialize ConvLSTM cell.\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_dim: int\n",
        "            Number of channels of input tensor.\n",
        "        hidden_dim: int\n",
        "            Number of channels of hidden state.\n",
        "        kernel_size: (int, int)\n",
        "            Size of the convolutional kernel.\n",
        "        bias: bool\n",
        "            Whether or not to add the bias.\n",
        "        \"\"\"\n",
        " \n",
        "        super(ConvLSTMCell, self).__init__()\n",
        " \n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        " \n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2 # 保证在传递过程中 （h,w）不变\n",
        "        self.bias = bias\n",
        " \n",
        "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
        "                              out_channels=4 * self.hidden_dim, # i门，f门，o门，g门放在一起计算，然后在split开\n",
        "                              kernel_size=self.kernel_size,\n",
        "                              padding=self.padding,\n",
        "                              bias=self.bias)\n",
        " \n",
        "    def forward(self, input_tensor, cur_state):\n",
        "        h_cur, c_cur = cur_state # 每个timestamp包含两个状态张量：h和c\n",
        " \n",
        "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis # 把输入张量与h状态张量沿通道维度串联\n",
        " \n",
        "        combined_conv = self.conv(combined) # i门，f门，o门，g门放在一起计算，然后在split开\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
        "        i = torch.sigmoid(cc_i)\n",
        "        f = torch.sigmoid(cc_f)\n",
        "        o = torch.sigmoid(cc_o)\n",
        "        g = torch.tanh(cc_g)\n",
        " \n",
        "        c_next = f * c_cur + i * g  # c状态张量更新\n",
        "        h_next = o * torch.tanh(c_next) # h状态张量更新\n",
        " \n",
        "        return h_next, c_next # 输出当前timestamp的两个状态张量\n",
        " \n",
        "    def init_hidden(self, batch_size, image_size):\n",
        "        \"\"\"\n",
        "        初始状态张量初始化.第一个timestamp的状态张量0初始化\n",
        "        :param batch_size:\n",
        "        :param image_size:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        height, width = image_size\n",
        "        init_h = torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
        "        init_c = torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
        "        return (init_h,init_c)\n",
        " \n",
        " \n",
        "class ConvLSTM(nn.Module):\n",
        " \n",
        "    \"\"\"\n",
        "    Parameters:参数介绍\n",
        "        input_dim: Number of channels in input# 输入张量的通道数\n",
        "        hidden_dim: Number of hidden channels # h,c两个状态张量的通道数，可以是一个列表\n",
        "        kernel_size: Size of kernel in convolutions # 卷积核的尺寸，默认所有层的卷积核尺寸都是一样的,也可以设定不通lstm层的卷积核尺寸不同\n",
        "        num_layers: Number of LSTM layers stacked on each other # 卷积层的层数，需要与len(hidden_dim)相等\n",
        "        batch_first: Whether or not dimension 0 is the batch or not\n",
        "        bias: Bias or no bias in Convolution\n",
        "        return_all_layers: Return the list of computations for all layers # 是否返回所有lstm层的h状态\n",
        "        Note: Will do same padding. # 相同的卷积核尺寸，相同的padding尺寸\n",
        "    Input:输入介绍\n",
        "        A tensor of size [B, T, C, H, W] or [T, B, C, H, W]# 需要是5维的\n",
        "    Output:输出介绍\n",
        "        返回的是两个列表：layer_output_list，last_state_list\n",
        "        列表0：layer_output_list--单层列表，每个元素表示一层LSTM层的输出h状态,每个元素的size=[B,T,hidden_dim,H,W]\n",
        "        列表1：last_state_list--双层列表，每个元素是一个二元列表[h,c],表示每一层的最后一个timestamp的输出状态[h,c],h.size=c.size = [B,hidden_dim,H,W]\n",
        "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
        "            0 - layer_output_list is the list of lists of length T of each output\n",
        "            1 - last_state_list is the list of last states\n",
        "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
        "    Example:使用示例\n",
        "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
        "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
        "        >> _, last_states = convlstm(x)\n",
        "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
        "    \"\"\"\n",
        " \n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
        "                 batch_first=False, bias=True, return_all_layers=False):\n",
        "        super(ConvLSTM, self).__init__()\n",
        " \n",
        "        self._check_kernel_size_consistency(kernel_size)\n",
        " \n",
        "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
        "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers) # 转为列表\n",
        "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers) # 转为列表\n",
        "        if not len(kernel_size) == len(hidden_dim) == num_layers: # 判断一致性\n",
        "            raise ValueError('Inconsistent list length.')\n",
        " \n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_first = batch_first\n",
        "        self.bias = bias\n",
        "        self.return_all_layers = return_all_layers\n",
        " \n",
        "        cell_list = []\n",
        "        for i in range(0, self.num_layers): # 多层LSTM设置\n",
        "            # 当前LSTM层的输入维度\n",
        "            # if i==0:\n",
        "            #     cur_input_dim = self.input_dim\n",
        "            # else:\n",
        "            #     cur_input_dim = self.hidden_dim[i - 1]\n",
        "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1] # 与上等价\n",
        "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
        "                                          hidden_dim=self.hidden_dim[i],\n",
        "                                          kernel_size=self.kernel_size[i],\n",
        "                                          bias=self.bias))\n",
        " \n",
        "        self.cell_list = nn.ModuleList(cell_list) # 把定义的多个LSTM层串联成网络模型\n",
        " \n",
        "    def forward(self, input_tensor, hidden_state=None):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_tensor: 5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
        "        hidden_state: todo\n",
        "            None. todo implement stateful\n",
        "        Returns\n",
        "        -------\n",
        "        last_state_list, layer_output\n",
        "        \"\"\"\n",
        "        if not self.batch_first:\n",
        "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
        "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
        " \n",
        "        # Implement stateful ConvLSTM\n",
        "        if hidden_state is not None:\n",
        "            raise NotImplementedError()\n",
        "        else:\n",
        "            # Since the init is done in forward. Can send image size here\n",
        "            b, _, _, h, w = input_tensor.size()  # 自动获取 b,h,w信息\n",
        "            hidden_state = self._init_hidden(batch_size=b,image_size=(h, w))\n",
        " \n",
        "        layer_output_list = []\n",
        "        last_state_list = []\n",
        " \n",
        "        seq_len = input_tensor.size(1) # 根据输入张量获取lstm的长度\n",
        "        cur_layer_input = input_tensor\n",
        " \n",
        "        for layer_idx in range(self.num_layers): # 逐层计算\n",
        " \n",
        "            h, c = hidden_state[layer_idx]\n",
        "            output_inner = []\n",
        "            for t in range(seq_len): # 逐个stamp计算\n",
        "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],cur_state=[h, c])\n",
        "                output_inner.append(h) # 第 layer_idx 层的第t个stamp的输出状态\n",
        " \n",
        "            layer_output = torch.stack(output_inner, dim=1) # 第 layer_idx 层的第所有stamp的输出状态串联\n",
        "            cur_layer_input = layer_output # 准备第layer_idx+1层的输入张量\n",
        " \n",
        "            layer_output_list.append(layer_output) # 当前层的所有timestamp的h状态的串联\n",
        "            last_state_list.append([h, c]) # 当前层的最后一个stamp的输出状态的[h,c]\n",
        " \n",
        "        if not self.return_all_layers:\n",
        "            layer_output_list = layer_output_list[-1:]\n",
        "            last_state_list = last_state_list[-1:]\n",
        " \n",
        "        return layer_output_list, last_state_list\n",
        " \n",
        "    def _init_hidden(self, batch_size, image_size):\n",
        "        \"\"\"\n",
        "        所有lstm层的第一个timestamp的输入状态0初始化\n",
        "        :param batch_size:\n",
        "        :param image_size:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        init_states = []\n",
        "        for i in range(self.num_layers):\n",
        "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
        "        return init_states\n",
        " \n",
        "    @staticmethod\n",
        "    def _check_kernel_size_consistency(kernel_size):\n",
        "        \"\"\"\n",
        "        检测输入的kernel_size是否符合要求，要求kernel_size的格式是list或tuple\n",
        "        :param kernel_size:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if not (isinstance(kernel_size, tuple) or\n",
        "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
        "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
        " \n",
        "    @staticmethod\n",
        "    def _extend_for_multilayer(param, num_layers):\n",
        "        \"\"\"\n",
        "        扩展到多层lstm情况\n",
        "        :param param:\n",
        "        :param num_layers:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if not isinstance(param, list):\n",
        "            param = [param] * num_layers\n",
        "        return param\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    data = torch.randn((5,6,3,30,30))\n",
        "    model = ConvLSTM(input_dim=3,\n",
        "                     hidden_dim=[64, 64, 128],\n",
        "                     kernel_size=[(3, 3),(5,5),(7,7)],\n",
        "                     num_layers=3,\n",
        "                     batch_first=True,\n",
        "                     bias = True,\n",
        "                     return_all_layers = True)\n",
        "    layer_output_list, last_state_list = model(data)\n",
        "    \n",
        "    last_layer_output = layer_output_list[-1]\n",
        "    last_layer_last_h,last_layer_last_c = last_state_list[-1]\n",
        "    \n",
        "    print(last_layer_output[:,-1,...]==last_layer_last_h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "注意事项：要注意hidden_dim,kernel_size,num_layers三个参数在LSTM层上的一致。即：\n",
        "\n",
        "len(kernel_size) == len(hidden_dim) == num_layers\n",
        "如果hidden_dim=64，kernel_size = (3,3), num_layers=3: 会搭建一个3层的convLSTM网络，每一层的隐状态都是64通道，kernel_size=(3,3)\n",
        "\n",
        "如果hidden_dim=[64,128,256]，kernel_size = (3,3), num_layers=3: 会搭建一个3层的convLSTM网络，各层的隐状态通道数分别是[64,128,256]，所有层的kernel_size==(3,3)\n",
        "\n",
        "如果hidden_dim=[64,128,256]，kernel_size = [(3,3),(5,5),(7,7)], num_layers=3: 会搭建一个3层的convLSTM网络，各层的隐状态通道数分别是[64,128,256]，各层的kernel_size分别是[(3,3),(5,5),(7,7)]\n",
        "\n",
        "https://blog.csdn.net/Strive_For_Future/article/details/115507864"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x1112f2c90>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        # return score, tag_seq\n",
        "        return tag_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "PARTICIPANT_ID = 1 #4348\n",
        "\n",
        "df_csv=pd.read_csv('/Users/apple/Desktop/deep_eye/dataset/input_right.csv',usecols=['participant_ID','Q_ID','text','fixation1'])\n",
        "df_csv=df_csv[(df_csv['participant_ID']==PARTICIPANT_ID)] # take 1st participant data\n",
        "df_csv=df_csv.drop(['participant_ID'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " [['沙罐', '置入', '不同', '濃度', '鹽水', '中', '所', '受', '浮力', '會', '如何', '相同', '濃度', '愈', '高', '所', '受', '浮力', '愈', '大', '濃度', '愈', '高', '所', '受', '浮力', '愈', '小'], [0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1]] \n",
            " ['沙罐', '置入', '不同', '濃度', '鹽水', '中', '所', '受', '浮力', '會', '如何', '相同', '濃度', '愈', '高', '所', '受', '浮力', '愈', '大', '濃度', '愈', '高', '所', '受', '浮力', '愈', '小'] \n",
            " [0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "q_index_list = sorted(list(set(df_csv['Q_ID'])))\n",
        "\n",
        "total_q_list = []\n",
        "for q in q_index_list:\n",
        "    df_q = df_csv[df_csv['Q_ID'] == q]\n",
        "    text_list = list(df_q['text'])\n",
        "    label_list = list(df_q['fixation1'])\n",
        "    q_list = [text_list, label_list]\n",
        "    total_q_list.append(q_list)\n",
        "\n",
        "print(\n",
        "    '\\n',total_q_list[0], # first data with text and label\n",
        "    '\\n',total_q_list[0][0], # first data's text\n",
        "    '\\n',total_q_list[0][1], # first data's label\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "real label：[0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1]\n",
            "untrained label prediction：[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "=============start BiLSTM+CRF model training=============\n",
            "the0th epoch Loss:20.18512725830078\n",
            "the50th epoch Loss:12.962231636047363\n",
            "the100th epoch Loss:8.370246887207031\n",
            "the150th epoch Loss:4.185600280761719\n",
            "=============trained model saved=============\n",
            "\n",
            "\n",
            "=============load trained model and predict=============\n",
            "trained label prediction：[0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "EMBEDDING_DIM = 4 #5\n",
        "HIDDEN_DIM = 4 #4\n",
        "\n",
        "train_num = int(len(q_index_list)*0.8)\n",
        "training_data = total_q_list[:train_num]\n",
        "testing_data = total_q_list[train_num:]\n",
        "\n",
        "# word_to_ix # unique word and its representing number\n",
        "word_to_ix = {}\n",
        "for sentence, tags in training_data:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "tag_to_ix = {0: 0, 1: 1, START_TAG: 2, STOP_TAG: 3}\n",
        "\n",
        "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# Check predictions before training\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
        "    print('real label：'+ str(precheck_tags.tolist()))\n",
        "    print('untrained label prediction：'+ str(model(precheck_sent)))\n",
        "\n",
        "print('=============start BiLSTM+CRF model training=============')\n",
        "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
        "for epoch in range(200):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    for sentence, tags in training_data:\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch%50 == 0:\n",
        "        print(f'the{epoch}th epoch Loss:{loss[0]}')\n",
        "\n",
        "\n",
        "output_path = 'ner_trained_model.cpt'\n",
        "torch.save(model, output_path)\n",
        "print('=============trained model saved=============\\n\\n')\n",
        "\n",
        "\n",
        "\n",
        "print('=============load trained model and predict=============')\n",
        "model_path = 'ner_trained_model.cpt'\n",
        "trained_ner_model = torch.load(model_path)\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    print('trained label prediction：' + str(model(precheck_sent)))\n",
        "\n",
        "# time spent : 3min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # accuracy from scratch\n",
        "# total_wrong = []\n",
        "# acccuracy_score = []\n",
        "# for q in range(47):\n",
        "#     precheck_sent = prepare_sequence(training_data[q][0], word_to_ix)\n",
        "#     predict = model(precheck_sent)\n",
        "#     answer = training_data[q][1]\n",
        "#     accuracy = sum([1 for i,j in zip(predict,answer) if i==j])/len(answer)\n",
        "#     print(str(q+1) + \" accuracy:\", round(accuracy,2))\n",
        "#     acccuracy_score.append(accuracy)\n",
        "#     wrong_list = [i for i,j in zip(predict,answer) if i!=j] # wrong answer\n",
        "#     total_wrong.append(wrong_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['沙罐', '置入', '不同', '濃度', '鹽水', '中', '所', '受', '浮力', '會', '如何', '相同', '濃度', '愈', '高', '所', '受', '浮力', '愈', '大', '濃度', '愈', '高', '所', '受', '浮力', '愈', '小']\n",
            "[0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1]\n",
            "[0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1]\n",
            "===============================\n",
            "['外加', '電池', '將', '碳棒', '及', '銅棒', '放入', '硫酸銅', '𥚃', '將', '發生', '什麼', '反應', '電解', '反應', '電池', '反應', '氧化還原', '反應']\n",
            "[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1]\n",
            "[0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1]\n",
            "===============================\n",
            "['以', '針筒', '吸取', '熱水', '並', '封住', '針頭', '將', '針頭', '活塞', '向', '外', '拉出', '裡面', '的', '水', '會', '有', '何', '現象', '水位', '不變', '水', '沸騰', '水', '向', '上', '四散', '各', '處']\n",
            "[0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0]\n",
            "===============================\n",
            "['雌蕊', '的', '花柱', '內', '有', '多少', '個', '花粉管', '一', '個', '兩', '個', '很多', '個']\n",
            "[0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0]\n",
            "[0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0]\n",
            "===============================\n",
            "['A', '和', 'B', '燈泡', '並聯', '並', '與', 'C', '燈泡', '串聯', '這', 'A', 'B', 'C', '三', '個', '燈泡', '的', '亮度', '大小', '何者', '正確', 'A', '>', 'B', '>', 'C', 'A', '=', 'B', '>', 'C', 'A', '=', 'B', '<', 'C']\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0]\n",
            "===============================\n",
            "['將', '罐子', '綁上', '乒乓球', '當', '往', '右', '移動', '罐子', '罐', '內', '乒乓球', '會', '有', '何', '現象', '向', '右', '跑', '向', '左', '跑', '停', '在', '原處', '不', '動']\n",
            "[0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0]\n",
            "[0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0]\n",
            "===============================\n",
            "['在', '兩', '個', '懸掛', '的', '氣球', '中間', '用', '吹風機', '吹氣', '請問', '氣球', '會', '有', '什麼', '變化', '兩', '球', '分開', '兩', '球', '靠近', '兩', '球', '位置', '不變']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
            "===============================\n",
            "['在', '鋅銅電池', '反應', '中', '檢流計', '指針', '的', '指向', '電子流向', '是', '向', '左', '不', '流動', '向', '右']\n",
            "[0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1]\n",
            "[0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1]\n",
            "===============================\n",
            "['在', '封閉', '針筒', '中', '置入', '一', '小', '氣球', '將', '活塞', '向', '外', '拉', '後', '氣球', '會', '有', '何', '變化', '氣球', '縮小', '氣球', '脹大', '氣球', '大小', '不變']\n",
            "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0]\n",
            "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0]\n",
            "===============================\n",
            "['同', '植株', '兩', '片', '葉子', '分別', '套入', '碳酸氫鈉', '與', '氫氧化鈉', '水溶液', '袋中', '光照', '數日', '後', '有無', '澱粉', '反應', '僅', '碳酸氫鈉', '組', '有', '澱粉', '反應', '僅', '氫氧化鈉', '組', '有', '澱粉', '反應', '兩', '者', '皆', '有', '澱粉', '反應']\n",
            "[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "===============================\n",
            "['砝碼', '置於', '一', '個', '和', '串聯', '二', '個', '相同', '的', '彈簧', '下', '不計', '彈簧', '重量', 'A', 'B', 'C', '彈簧', '伸長量', '一', '個', '彈簧', 'C', '比較', '長', '串聯', '之', '彈簧', 'A', '和', 'B', '比較', '長', 'A', '=', 'B', '=', 'C']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
            "===============================\n",
            "['將', '金屬', '銅', '放', '在', '硫酸銅', '溶液', '中', '會', '發生', '什麼', '反應', '產生', '電流', '不', '產生', '電流', '產生', '新', '物質']\n",
            "[0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1]\n",
            "[0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0]\n",
            "===============================\n",
            "['水', '沸騰', '後', '將', '火', '移開', '封住', '瓶口', '澆', '冷水', '瓶', '內', '水', '會', '如何', '沸騰', '凝結', '不會', '改變']\n",
            "[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
            "[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
            "===============================\n",
            "['攀藤植物', '沒有', '支撐物', '時', '會', '如何', '生長', '會', '依', '在', '地面上', '長', '會', '直直', '向', '上', '長', '會', '旋轉', '向', '上', '長']\n",
            "[0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0]\n",
            "[0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0]\n",
            "===============================\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# training predict score\n",
        "total_result_list = []\n",
        "for q in range(len(training_data)):\n",
        "    precheck_sent = prepare_sequence(training_data[q][0], word_to_ix)\n",
        "    predict = model(precheck_sent)\n",
        "    answer = training_data[q][1]\n",
        "    print(training_data[q][0])\n",
        "    print(predict)\n",
        "    print(answer)\n",
        "    print(\"===============================\")\n",
        "    accuracy = accuracy_score(answer, predict)\n",
        "    precision = precision_score(answer, predict)\n",
        "    recall = recall_score(answer, predict)\n",
        "    f1score = f1_score(answer, predict)\n",
        "    result_list = [accuracy, precision, recall, f1score]\n",
        "    result_list = [round(elem, 2) for elem in result_list]\n",
        "    total_result_list.append(result_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.96</td>\n",
              "      <td>0.94</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.95</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.97</td>\n",
              "      <td>0.93</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.86</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.86</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.92</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.83</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.97</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.85</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.95</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.95</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    accuracy  precision  recall  f1score\n",
              "0       0.96       0.94    1.00     0.97\n",
              "1       0.95       1.00    0.92     0.96\n",
              "2       0.97       0.93    1.00     0.96\n",
              "3       0.86       0.80    0.80     0.80\n",
              "4       0.86       0.86    0.80     0.83\n",
              "5       1.00       1.00    1.00     1.00\n",
              "6       0.92       0.82    1.00     0.90\n",
              "7       1.00       1.00    1.00     1.00\n",
              "8       1.00       1.00    1.00     1.00\n",
              "9       0.83       0.77    0.77     0.77\n",
              "10      0.97       1.00    0.92     0.96\n",
              "11      0.85       0.83    0.91     0.87\n",
              "12      1.00       1.00    1.00     1.00\n",
              "13      0.95       1.00    0.91     0.95"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_result_df = pd.DataFrame(total_result_list)\n",
        "total_result_df.columns = ['accuracy', 'precision', 'recall', 'f1score']\n",
        "total_result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "accuracy     0.937143\n",
              "precision    0.925000\n",
              "recall       0.930714\n",
              "f1score      0.926429\n",
              "dtype: float64"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_result_df.mean(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "testing data show"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for testing data\n",
        "test_word_to_ix = {}\n",
        "for sentence, tags in testing_data:\n",
        "    for word in sentence:\n",
        "        if word not in test_word_to_ix:\n",
        "            test_word_to_ix[word] = len(test_word_to_ix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['細長', '的', '玻璃管', '裝', '水', '1033.6', 'cm', '同時', '拿掉', '玻璃管', '上', '和', '下', '管', '蓋', '水', '會', '跑出來', '嗎', '會', '不會', '只', '會', '跑出', '一點點']\n",
            "[0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0]\n",
            "===============================\n",
            "['未成熟', '的', '香蕉', '和', '成熟', '香蕉', '加', '碘液', '後', '哪', '個', '會', '比較', '黑', '呢', '未成熟', '成熟', '一樣', '黑']\n",
            "[0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1]\n",
            "===============================\n",
            "['在', '抽氣', '密封罐', '內', '放置', '一', '手套', '將', '罐', '內', '氣體', '抽出', '後', '手套', '會', '有', '何', '變化', '手套', '縮小', '手套', '脹大', '手套', '不變']\n",
            "[0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1]\n",
            "[0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0]\n",
            "===============================\n",
            "['砝碼', '置於', '串聯', '和', '並聯', '的', '二', '個', '相同', '的', '彈簧', '下', '不計', '彈簧', '重量', 'A', 'B', 'C', 'D', '四', '個', '彈簧', '伸長量', '並聯', '之', '彈簧', 'C', 'D', '比較', '長', '串聯', '之', '彈簧', 'A', 'B', '比較', '長', 'A', '=', 'B', '=', 'C', '=', 'D']\n",
            "[0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "===============================\n"
          ]
        }
      ],
      "source": [
        "total_result_list = []\n",
        "for q in range(len(testing_data)):\n",
        "    precheck_sent = prepare_sequence(testing_data[q][0], test_word_to_ix)\n",
        "    predict = model(precheck_sent)\n",
        "    answer = testing_data[q][1]\n",
        "    print(testing_data[q][0])\n",
        "    print(predict)\n",
        "    print(answer)\n",
        "    print(\"===============================\")\n",
        "    accuracy = accuracy_score(answer, predict)\n",
        "    precision = precision_score(answer, predict)\n",
        "    recall = recall_score(answer, predict)\n",
        "    f1score = f1_score(answer, predict)\n",
        "    result_list = [accuracy, precision, recall, f1score]\n",
        "    result_list = [round(elem, 2) for elem in result_list]\n",
        "    total_result_list.append(result_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.44</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.42</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.42</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.52</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  precision  recall  f1score\n",
              "0      0.44       0.00    0.00     0.00\n",
              "1      0.42       0.33    0.57     0.42\n",
              "2      0.42       0.31    0.62     0.42\n",
              "3      0.52       0.08    0.09     0.09"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_result_df = pd.DataFrame(total_result_list)\n",
        "total_result_df.columns = ['accuracy', 'precision', 'recall', 'f1score']\n",
        "total_result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "accuracy     0.4500\n",
              "precision    0.1800\n",
              "recall       0.3200\n",
              "f1score      0.2325\n",
              "dtype: float64"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_result_df.mean(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # draw confusion matrix\n",
        "# import matplotlib.pyplot as plt\n",
        "# conf_matrix = confusion_matrix(y_true=answer, y_pred=predict)\n",
        "# fig, ax = plt.subplots(figsize=(5, 5))\n",
        "# ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n",
        "# for i in range(conf_matrix.shape[0]):\n",
        "#     for j in range(conf_matrix.shape[1]):\n",
        "#         ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
        "\n",
        "# plt.xlabel('Predictions', fontsize=18)\n",
        "# plt.ylabel('Actuals', fontsize=18)\n",
        "# plt.title('Confusion Matrix', fontsize=18)\n",
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deepeye_env",
      "language": "python",
      "name": "deepeye_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
