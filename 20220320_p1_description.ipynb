{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1score</th>\n",
              "      <th>P_ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.89</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.76</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.64</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.92</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.88</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.63</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.12</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.88</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.85</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.86</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.80</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.68</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.67</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.86</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.83</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.85</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.84</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.77</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.47</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.84</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.81</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.55</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.40</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.86</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.90</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.67</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.81</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.67</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.88</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.67</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.88</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.89</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.71</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.78</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.82</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.89</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.80</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    accuracy  precision  recall  f1score  P_ID\n",
              "5       0.89       0.86    0.67     0.75     1\n",
              "7       0.76       0.80    0.53     0.64     1\n",
              "11      0.92       1.00    0.78     0.88     1\n",
              "14      0.63       0.50    0.07     0.12     1\n",
              "16      0.88       1.00    0.70     0.82     1\n",
              "18      0.85       0.78    0.88     0.82     1\n",
              "19      0.86       1.00    0.67     0.80     1\n",
              "20      0.68       0.86    0.55     0.67     1\n",
              "21      0.86       0.91    0.77     0.83     1\n",
              "22      0.85       1.00    0.73     0.84     1\n",
              "23      0.77       1.00    0.31     0.47     1\n",
              "25      0.84       0.92    0.73     0.81     1\n",
              "26      0.55       0.75    0.27     0.40     1\n",
              "29      0.86       1.00    0.33     0.50     1\n",
              "32      0.90       0.75    0.60     0.67     1\n",
              "33      0.81       0.80    0.57     0.67     1\n",
              "35      0.88       0.75    0.60     0.67     1\n",
              "39      0.88       0.90    0.75     0.82     1\n",
              "40      0.89       0.60    0.86     0.71     1\n",
              "42      0.78       0.82    0.82     0.82     1\n",
              "43      0.82       0.80    0.36     0.50     1\n",
              "44      0.89       0.86    0.75     0.80     1"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"/Users/apple/Desktop/deep_eye/dataset/output_score_person/score_1.csv\").iloc[:-1,1:]\n",
        "df['P_ID'] = [1]*df.shape[0]\n",
        "df = df[df[df<0.8].notnull().any(axis=1)]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "accuracy     0.94\n",
              "precision    1.00\n",
              "recall       0.88\n",
              "f1score      0.93\n",
              "P_ID         1.00\n",
              "Name: 46, dtype: float64"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[46]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1score</th>\n",
              "      <th>P_ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.89</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.76</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.64</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.92</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.88</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.63</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.12</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.88</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.85</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.86</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.80</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.68</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.67</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.86</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.83</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.85</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.84</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.77</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.47</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.84</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.81</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.55</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.40</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.86</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.90</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.67</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.81</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.67</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.88</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.67</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.88</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.89</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.71</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.78</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.82</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.89</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.80</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    accuracy  precision  recall  f1score  P_ID\n",
              "5       0.89       0.86    0.67     0.75     1\n",
              "7       0.76       0.80    0.53     0.64     1\n",
              "11      0.92       1.00    0.78     0.88     1\n",
              "14      0.63       0.50    0.07     0.12     1\n",
              "16      0.88       1.00    0.70     0.82     1\n",
              "18      0.85       0.78    0.88     0.82     1\n",
              "19      0.86       1.00    0.67     0.80     1\n",
              "20      0.68       0.86    0.55     0.67     1\n",
              "21      0.86       0.91    0.77     0.83     1\n",
              "22      0.85       1.00    0.73     0.84     1\n",
              "23      0.77       1.00    0.31     0.47     1\n",
              "25      0.84       0.92    0.73     0.81     1\n",
              "26      0.55       0.75    0.27     0.40     1\n",
              "29      0.86       1.00    0.33     0.50     1\n",
              "32      0.90       0.75    0.60     0.67     1\n",
              "33      0.81       0.80    0.57     0.67     1\n",
              "35      0.88       0.75    0.60     0.67     1\n",
              "39      0.88       0.90    0.75     0.82     1\n",
              "40      0.89       0.60    0.86     0.71     1\n",
              "42      0.78       0.82    0.82     0.82     1\n",
              "43      0.82       0.80    0.36     0.50     1\n",
              "44      0.89       0.86    0.75     0.80     1"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[df[df<0.8].notnull().any(axis=1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.878085</td>\n",
              "      <td>0.898085</td>\n",
              "      <td>0.768723</td>\n",
              "      <td>0.810638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.897447</td>\n",
              "      <td>0.897872</td>\n",
              "      <td>0.860213</td>\n",
              "      <td>0.871064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.881064</td>\n",
              "      <td>0.899362</td>\n",
              "      <td>0.810851</td>\n",
              "      <td>0.838723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.887234</td>\n",
              "      <td>0.893191</td>\n",
              "      <td>0.847447</td>\n",
              "      <td>0.864255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.867021</td>\n",
              "      <td>0.922979</td>\n",
              "      <td>0.677021</td>\n",
              "      <td>0.760426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.916809</td>\n",
              "      <td>0.935319</td>\n",
              "      <td>0.860213</td>\n",
              "      <td>0.891915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.875319</td>\n",
              "      <td>0.937660</td>\n",
              "      <td>0.802979</td>\n",
              "      <td>0.850851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.911277</td>\n",
              "      <td>0.871702</td>\n",
              "      <td>0.832340</td>\n",
              "      <td>0.838085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.879149</td>\n",
              "      <td>0.940426</td>\n",
              "      <td>0.820851</td>\n",
              "      <td>0.867660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.907021</td>\n",
              "      <td>0.909787</td>\n",
              "      <td>0.834043</td>\n",
              "      <td>0.863191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.885319</td>\n",
              "      <td>0.938936</td>\n",
              "      <td>0.757021</td>\n",
              "      <td>0.829362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.781702</td>\n",
              "      <td>0.873191</td>\n",
              "      <td>0.644894</td>\n",
              "      <td>0.708511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.880213</td>\n",
              "      <td>0.893191</td>\n",
              "      <td>0.826596</td>\n",
              "      <td>0.850213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.897872</td>\n",
              "      <td>0.860000</td>\n",
              "      <td>0.873404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.904681</td>\n",
              "      <td>0.906809</td>\n",
              "      <td>0.818511</td>\n",
              "      <td>0.851064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.887660</td>\n",
              "      <td>0.906383</td>\n",
              "      <td>0.857234</td>\n",
              "      <td>0.870213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.899362</td>\n",
              "      <td>0.893617</td>\n",
              "      <td>0.803191</td>\n",
              "      <td>0.829574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.884043</td>\n",
              "      <td>0.913191</td>\n",
              "      <td>0.816596</td>\n",
              "      <td>0.847021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.882979</td>\n",
              "      <td>0.798298</td>\n",
              "      <td>0.536596</td>\n",
              "      <td>0.618936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.848936</td>\n",
              "      <td>0.893830</td>\n",
              "      <td>0.782340</td>\n",
              "      <td>0.821489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.891064</td>\n",
              "      <td>0.862979</td>\n",
              "      <td>0.754468</td>\n",
              "      <td>0.794894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.862128</td>\n",
              "      <td>0.906170</td>\n",
              "      <td>0.784468</td>\n",
              "      <td>0.830000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.839574</td>\n",
              "      <td>0.922553</td>\n",
              "      <td>0.659149</td>\n",
              "      <td>0.751489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.916809</td>\n",
              "      <td>0.910426</td>\n",
              "      <td>0.878085</td>\n",
              "      <td>0.884468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.908298</td>\n",
              "      <td>0.928511</td>\n",
              "      <td>0.882553</td>\n",
              "      <td>0.899787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.890638</td>\n",
              "      <td>0.891702</td>\n",
              "      <td>0.640638</td>\n",
              "      <td>0.722340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.909574</td>\n",
              "      <td>0.915532</td>\n",
              "      <td>0.880426</td>\n",
              "      <td>0.892553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.874468</td>\n",
              "      <td>0.923617</td>\n",
              "      <td>0.814468</td>\n",
              "      <td>0.855745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.897660</td>\n",
              "      <td>0.842766</td>\n",
              "      <td>0.910851</td>\n",
              "      <td>0.871277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.910851</td>\n",
              "      <td>0.895532</td>\n",
              "      <td>0.880213</td>\n",
              "      <td>0.874255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.897872</td>\n",
              "      <td>0.779787</td>\n",
              "      <td>0.703191</td>\n",
              "      <td>0.710213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.927234</td>\n",
              "      <td>0.834255</td>\n",
              "      <td>0.799362</td>\n",
              "      <td>0.796170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.940426</td>\n",
              "      <td>0.747447</td>\n",
              "      <td>0.655106</td>\n",
              "      <td>0.679362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.902553</td>\n",
              "      <td>0.722979</td>\n",
              "      <td>0.686809</td>\n",
              "      <td>0.677021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.917872</td>\n",
              "      <td>0.659787</td>\n",
              "      <td>0.517447</td>\n",
              "      <td>0.563617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.955957</td>\n",
              "      <td>0.655745</td>\n",
              "      <td>0.547447</td>\n",
              "      <td>0.584681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.930638</td>\n",
              "      <td>0.783191</td>\n",
              "      <td>0.663191</td>\n",
              "      <td>0.703617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.897021</td>\n",
              "      <td>0.782128</td>\n",
              "      <td>0.550638</td>\n",
              "      <td>0.618936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.943617</td>\n",
              "      <td>0.904894</td>\n",
              "      <td>0.803830</td>\n",
              "      <td>0.834468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.971702</td>\n",
              "      <td>0.609787</td>\n",
              "      <td>0.537447</td>\n",
              "      <td>0.558085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.905745</td>\n",
              "      <td>0.827234</td>\n",
              "      <td>0.681277</td>\n",
              "      <td>0.714894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.902128</td>\n",
              "      <td>0.757660</td>\n",
              "      <td>0.645745</td>\n",
              "      <td>0.685957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.941277</td>\n",
              "      <td>0.565532</td>\n",
              "      <td>0.491489</td>\n",
              "      <td>0.516170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.912340</td>\n",
              "      <td>0.764255</td>\n",
              "      <td>0.676809</td>\n",
              "      <td>0.693404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.888298</td>\n",
              "      <td>0.729574</td>\n",
              "      <td>0.765106</td>\n",
              "      <td>0.730851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.873617</td>\n",
              "      <td>0.721702</td>\n",
              "      <td>0.621277</td>\n",
              "      <td>0.633404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.922340</td>\n",
              "      <td>0.639787</td>\n",
              "      <td>0.534681</td>\n",
              "      <td>0.554681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.937447</td>\n",
              "      <td>0.470213</td>\n",
              "      <td>0.392553</td>\n",
              "      <td>0.415319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.948723</td>\n",
              "      <td>0.782128</td>\n",
              "      <td>0.700213</td>\n",
              "      <td>0.725745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.885745</td>\n",
              "      <td>0.801915</td>\n",
              "      <td>0.607021</td>\n",
              "      <td>0.671064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0.949149</td>\n",
              "      <td>0.751702</td>\n",
              "      <td>0.707021</td>\n",
              "      <td>0.714468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>0.918936</td>\n",
              "      <td>0.779574</td>\n",
              "      <td>0.625319</td>\n",
              "      <td>0.670213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>0.913404</td>\n",
              "      <td>0.732979</td>\n",
              "      <td>0.831277</td>\n",
              "      <td>0.764468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>0.931277</td>\n",
              "      <td>0.789149</td>\n",
              "      <td>0.749149</td>\n",
              "      <td>0.758511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>0.917660</td>\n",
              "      <td>0.812553</td>\n",
              "      <td>0.757021</td>\n",
              "      <td>0.767872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0.896170</td>\n",
              "      <td>0.668936</td>\n",
              "      <td>0.547021</td>\n",
              "      <td>0.586596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>0.911489</td>\n",
              "      <td>0.767021</td>\n",
              "      <td>0.699149</td>\n",
              "      <td>0.718723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>0.922766</td>\n",
              "      <td>0.780000</td>\n",
              "      <td>0.726383</td>\n",
              "      <td>0.735106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>0.942979</td>\n",
              "      <td>0.815532</td>\n",
              "      <td>0.759574</td>\n",
              "      <td>0.773404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>0.873830</td>\n",
              "      <td>0.879574</td>\n",
              "      <td>0.564468</td>\n",
              "      <td>0.652340</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    accuracy  precision    recall   f1score\n",
              "0   0.878085   0.898085  0.768723  0.810638\n",
              "1   0.897447   0.897872  0.860213  0.871064\n",
              "2   0.881064   0.899362  0.810851  0.838723\n",
              "3   0.887234   0.893191  0.847447  0.864255\n",
              "4   0.867021   0.922979  0.677021  0.760426\n",
              "5   0.916809   0.935319  0.860213  0.891915\n",
              "6   0.875319   0.937660  0.802979  0.850851\n",
              "7   0.911277   0.871702  0.832340  0.838085\n",
              "8   0.879149   0.940426  0.820851  0.867660\n",
              "9   0.907021   0.909787  0.834043  0.863191\n",
              "10  0.885319   0.938936  0.757021  0.829362\n",
              "11  0.781702   0.873191  0.644894  0.708511\n",
              "12  0.880213   0.893191  0.826596  0.850213\n",
              "13  0.900000   0.897872  0.860000  0.873404\n",
              "14  0.904681   0.906809  0.818511  0.851064\n",
              "15  0.887660   0.906383  0.857234  0.870213\n",
              "16  0.899362   0.893617  0.803191  0.829574\n",
              "17  0.884043   0.913191  0.816596  0.847021\n",
              "18  0.882979   0.798298  0.536596  0.618936\n",
              "19  0.848936   0.893830  0.782340  0.821489\n",
              "20  0.891064   0.862979  0.754468  0.794894\n",
              "21  0.862128   0.906170  0.784468  0.830000\n",
              "22  0.839574   0.922553  0.659149  0.751489\n",
              "23  0.916809   0.910426  0.878085  0.884468\n",
              "24  0.908298   0.928511  0.882553  0.899787\n",
              "25  0.890638   0.891702  0.640638  0.722340\n",
              "26  0.909574   0.915532  0.880426  0.892553\n",
              "27  0.874468   0.923617  0.814468  0.855745\n",
              "28  0.897660   0.842766  0.910851  0.871277\n",
              "29  0.910851   0.895532  0.880213  0.874255\n",
              "30  0.897872   0.779787  0.703191  0.710213\n",
              "31  0.927234   0.834255  0.799362  0.796170\n",
              "32  0.940426   0.747447  0.655106  0.679362\n",
              "33  0.902553   0.722979  0.686809  0.677021\n",
              "34  0.917872   0.659787  0.517447  0.563617\n",
              "35  0.955957   0.655745  0.547447  0.584681\n",
              "36  0.930638   0.783191  0.663191  0.703617\n",
              "37  0.897021   0.782128  0.550638  0.618936\n",
              "38  0.943617   0.904894  0.803830  0.834468\n",
              "39  0.971702   0.609787  0.537447  0.558085\n",
              "40  0.905745   0.827234  0.681277  0.714894\n",
              "41  0.902128   0.757660  0.645745  0.685957\n",
              "42  0.941277   0.565532  0.491489  0.516170\n",
              "43  0.912340   0.764255  0.676809  0.693404\n",
              "44  0.888298   0.729574  0.765106  0.730851\n",
              "45  0.873617   0.721702  0.621277  0.633404\n",
              "46  0.922340   0.639787  0.534681  0.554681\n",
              "47  0.937447   0.470213  0.392553  0.415319\n",
              "48  0.948723   0.782128  0.700213  0.725745\n",
              "49  0.885745   0.801915  0.607021  0.671064\n",
              "50  0.949149   0.751702  0.707021  0.714468\n",
              "51  0.918936   0.779574  0.625319  0.670213\n",
              "52  0.913404   0.732979  0.831277  0.764468\n",
              "53  0.931277   0.789149  0.749149  0.758511\n",
              "54  0.917660   0.812553  0.757021  0.767872\n",
              "55  0.896170   0.668936  0.547021  0.586596\n",
              "56  0.911489   0.767021  0.699149  0.718723\n",
              "57  0.922766   0.780000  0.726383  0.735106\n",
              "58  0.942979   0.815532  0.759574  0.773404\n",
              "59  0.873830   0.879574  0.564468  0.652340"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_total_60 = pd.read_csv(\"/Users/apple/Desktop/deep_eye/dataset/output_score_person/p_mean_score_60.csv\").iloc[:,1:]\n",
        "df_total_60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>P_ID</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>40</td>\n",
              "      <td>0.971702</td>\n",
              "      <td>0.609787</td>\n",
              "      <td>0.537447</td>\n",
              "      <td>0.558085</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    P_ID  accuracy  precision    recall   f1score\n",
              "39    40  0.971702   0.609787  0.537447  0.558085"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_total_60[df_total_60.accuracy == max(df_total_60.accuracy)]\n",
        "df_total_60[df_total_60.recall == max(df_total_60.recall)]\n",
        "df_total_60[df_total_60.precision == max(df_total_60.precision)]\n",
        "df_total_60[df_total_60.f1score == max(df_total_60.f1score)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "24    0.904787\n",
              "dtype: float64"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean_person = df_total_60.mean(axis=1)\n",
        "mean_person[mean_person == mean_person.max()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x121c28bd0>"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        # return score, tag_seq\n",
        "        return tag_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "PARTICIPANT_ID = 24\n",
        "\n",
        "df_csv=pd.read_csv('/Users/apple/Desktop/deep_eye/dataset/input_data.csv',usecols=['participant_ID','Q_ID','text','fixation1'])\n",
        "df_csv=df_csv[(df_csv['participant_ID']==PARTICIPANT_ID)] # take 1st participant data\n",
        "df_csv=df_csv.drop(['participant_ID'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " [['', '', '', '', '', '50', 'cc', '', '', '50', 'cc', '', '', '', '', '', 'cc', '', '100', 'cc', '', '100', 'cc', '', '100', 'cc'], [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0]] \n",
            " ['', '', '', '', '', '50', 'cc', '', '', '50', 'cc', '', '', '', '', '', 'cc', '', '100', 'cc', '', '100', 'cc', '', '100', 'cc'] \n",
            " [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0]\n"
          ]
        }
      ],
      "source": [
        "q_index_list = sorted(list(set(df_csv['Q_ID'])))\n",
        "\n",
        "total_q_list = []\n",
        "for q in q_index_list:\n",
        "    df_q = df_csv[df_csv['Q_ID'] == q]\n",
        "    text_list = list(df_q['text'])\n",
        "    label_list = list(df_q['fixation1'])\n",
        "    q_list = [text_list, label_list]\n",
        "    total_q_list.append(q_list)\n",
        "\n",
        "print(\n",
        "    '\\n',total_q_list[0], # first data with text and label\n",
        "    '\\n',total_q_list[0][0], # first data's text\n",
        "    '\\n',total_q_list[0][1], # first data's label\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_data = total_q_list\n",
        "\n",
        "# word_to_ix # unique word and its representing number\n",
        "word_to_ix = {}\n",
        "for sentence, tags in training_data:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load trained model and predict\n",
        "model_path = '/Users/apple/Desktop/deep_eye/dataset/output_score_person/ner_trained_model_24.cpt'\n",
        "model = torch.load(model_path)\n",
        "# with torch.no_grad():\n",
        "#     precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "#     print('trained label prediction' + str(model(precheck_sent)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'LSTM' object has no attribute '_flat_weights'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/Users/apple/Desktop/deep_eye/code/20200320_p1_description.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/apple/Desktop/deep_eye/code/20200320_p1_description.ipynb#ch0000010?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(training_data)):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/apple/Desktop/deep_eye/code/20200320_p1_description.ipynb#ch0000010?line=6'>7</a>\u001b[0m     precheck_sent \u001b[39m=\u001b[39m prepare_sequence(training_data[q][\u001b[39m0\u001b[39m], word_to_ix)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/apple/Desktop/deep_eye/code/20200320_p1_description.ipynb#ch0000010?line=7'>8</a>\u001b[0m     predict \u001b[39m=\u001b[39m model(precheck_sent)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/apple/Desktop/deep_eye/code/20200320_p1_description.ipynb#ch0000010?line=8'>9</a>\u001b[0m     answer \u001b[39m=\u001b[39m training_data[q][\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apple/Desktop/deep_eye/code/20200320_p1_description.ipynb#ch0000010?line=9'>10</a>\u001b[0m     \u001b[39mprint\u001b[39m(training_data[q][\u001b[39m0\u001b[39m])\n",
            "File \u001b[0;32m~/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32m/Users/apple/Desktop/deep_eye/code/20200320_p1_description.ipynb Cell 11'\u001b[0m in \u001b[0;36mBiLSTM_CRF.forward\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/apple/Desktop/deep_eye/code/20200320_p1_description.ipynb#ch0000005?line=150'>151</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, sentence):  \u001b[39m# dont confuse this with _forward_alg above.\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/apple/Desktop/deep_eye/code/20200320_p1_description.ipynb#ch0000005?line=151'>152</a>\u001b[0m     \u001b[39m# Get the emission scores from the BiLSTM\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/apple/Desktop/deep_eye/code/20200320_p1_description.ipynb#ch0000005?line=152'>153</a>\u001b[0m     lstm_feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_lstm_features(sentence)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/apple/Desktop/deep_eye/code/20200320_p1_description.ipynb#ch0000005?line=154'>155</a>\u001b[0m     \u001b[39m# Find the best path, given the features.\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/apple/Desktop/deep_eye/code/20200320_p1_description.ipynb#ch0000005?line=155'>156</a>\u001b[0m     score, tag_seq \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_viterbi_decode(lstm_feats)\n",
            "\u001b[1;32m/Users/apple/Desktop/deep_eye/code/20200320_p1_description.ipynb Cell 11'\u001b[0m in \u001b[0;36mBiLSTM_CRF._get_lstm_features\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apple/Desktop/deep_eye/code/20200320_p1_description.ipynb#ch0000005?line=83'>84</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_hidden()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apple/Desktop/deep_eye/code/20200320_p1_description.ipynb#ch0000005?line=84'>85</a>\u001b[0m embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword_embeds(sentence)\u001b[39m.\u001b[39mview(\u001b[39mlen\u001b[39m(sentence), \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/apple/Desktop/deep_eye/code/20200320_p1_description.ipynb#ch0000005?line=85'>86</a>\u001b[0m lstm_out, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(embeds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apple/Desktop/deep_eye/code/20200320_p1_description.ipynb#ch0000005?line=86'>87</a>\u001b[0m lstm_out \u001b[39m=\u001b[39m lstm_out\u001b[39m.\u001b[39mview(\u001b[39mlen\u001b[39m(sentence), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_dim)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apple/Desktop/deep_eye/code/20200320_p1_description.ipynb#ch0000005?line=87'>88</a>\u001b[0m lstm_feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden2tag(lstm_out)\n",
            "File \u001b[0;32m~/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/rnn.py:761\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/rnn.py?line=758'>759</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/rnn.py?line=759'>760</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/rnn.py?line=760'>761</a>\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[1;32m    <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/rnn.py?line=761'>762</a>\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n\u001b[1;32m    <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/rnn.py?line=762'>763</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/rnn.py?line=763'>764</a>\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/rnn.py?line=764'>765</a>\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
            "File \u001b[0;32m~/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1182'>1183</a>\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1183'>1184</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1184'>1185</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   <a href='file:///Users/apple/Documents/deepeye_env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1185'>1186</a>\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LSTM' object has no attribute '_flat_weights'"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# training predict score\n",
        "total_result_list = []\n",
        "for q in range(len(training_data)):\n",
        "    precheck_sent = prepare_sequence(training_data[q][0], word_to_ix)\n",
        "    predict = model(precheck_sent)\n",
        "    answer = training_data[q][1]\n",
        "    print(training_data[q][0])\n",
        "    print(predict)\n",
        "    print(answer)\n",
        "    \n",
        "    accuracy = accuracy_score(answer, predict)\n",
        "    precision = precision_score(answer, predict)\n",
        "    recall = recall_score(answer, predict)\n",
        "    f1score = f1_score(answer, predict)\n",
        "    print(\n",
        "        \"\\nacc:\",accuracy, \n",
        "        \"\\nprec:\",precision,\n",
        "        \"rec:\",recall, \n",
        "        \"f1:\",f1score\n",
        "    )\n",
        "    print(\"===============================\")\n",
        "    # result_list = [accuracy, precision, recall, f1score]\n",
        "    # result_list = [round(elem, 2) for elem in result_list]\n",
        "\n",
        "    # total_result_list.append(result_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "real label[0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0]\n",
            "untrained label prediction[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "=============start BiLSTM+CRF model training=============\n",
            "the0th epoch Loss:23.633771896362305\n",
            "the50th epoch Loss:22.69854736328125\n",
            "the100th epoch Loss:21.55597496032715\n",
            "the150th epoch Loss:17.643531799316406\n",
            "=============trained model saved=============\n",
            "\n",
            "\n",
            "=============load trained model and predict=============\n",
            "trained label prediction[0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0]\n"
          ]
        }
      ],
      "source": [
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "EMBEDDING_DIM = 4 #5\n",
        "HIDDEN_DIM = 4 #4\n",
        "\n",
        "train_num = int(len(q_index_list)*0.8)\n",
        "training_data = total_q_list[:train_num]\n",
        "testing_data = total_q_list[train_num:]\n",
        "\n",
        "# word_to_ix # unique word and its representing number\n",
        "word_to_ix = {}\n",
        "for sentence, tags in training_data:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "tag_to_ix = {0: 0, 1: 1, START_TAG: 2, STOP_TAG: 3}\n",
        "\n",
        "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# Check predictions before training\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
        "    print('real label'+ str(precheck_tags.tolist()))\n",
        "    print('untrained label prediction'+ str(model(precheck_sent)))\n",
        "\n",
        "print('=============start BiLSTM+CRF model training=============')\n",
        "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
        "for epoch in range(200):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    for sentence, tags in training_data:\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch%50 == 0:\n",
        "        print(f'the{epoch}th epoch Loss:{loss[0]}')\n",
        "\n",
        "\n",
        "output_path = 'ner_trained_model.cpt'\n",
        "torch.save(model, output_path)\n",
        "print('=============trained model saved=============\\n\\n')\n",
        "\n",
        "\n",
        "\n",
        "print('=============load trained model and predict=============')\n",
        "model_path = 'ner_trained_model.cpt'\n",
        "trained_ner_model = torch.load(model_path)\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    print('trained label prediction' + str(model(precheck_sent)))\n",
        "\n",
        "# time spent : 3min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # accuracy from scratch\n",
        "# total_wrong = []\n",
        "# acccuracy_score = []\n",
        "# for q in range(47):\n",
        "#     precheck_sent = prepare_sequence(training_data[q][0], word_to_ix)\n",
        "#     predict = model(precheck_sent)\n",
        "#     answer = training_data[q][1]\n",
        "#     accuracy = sum([1 for i,j in zip(predict,answer) if i==j])/len(answer)\n",
        "#     print(str(q+1) + \" accuracy:\", round(accuracy,2))\n",
        "#     acccuracy_score.append(accuracy)\n",
        "#     wrong_list = [i for i,j in zip(predict,answer) if i!=j] # wrong answer\n",
        "#     total_wrong.append(wrong_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['', '', '', '', '', '50', 'cc', '', '', '50', 'cc', '', '', '', '', '', 'cc', '', '100', 'cc', '', '100', 'cc', '', '100', 'cc']\n",
            "[0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0]\n",
            "[0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0]\n",
            "[0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0]\n",
            "[0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0]\n",
            "[0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '5', '>', '3', '>', '1', '1', '=', '3', '=', '5', '1', '>', '3', '>', '5']\n",
            "[0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', 'A', 'B', 'C', '', '', '', '', '', 'A', '', '', '', '', '', 'B', '', 'C', '', '', 'A', '=', 'B', '=', 'C']\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', 'A', '', '', '', 'B', '', 'C', '', '', '', '', '', '', '', '']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1]\n",
            "===============================\n",
            "['', '', '', '', '', '1033.6', 'cm', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0]\n",
            "[0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'A', '', '', 'B', '', '', 'A', 'B', '', '']\n",
            "[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1]\n",
            "[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
            "[0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0]\n",
            "[0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0]\n",
            "[0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
            "[1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0]\n",
            "[0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0]\n",
            "===============================\n",
            "['A', '', 'B', '', '', 'C', '', 'D', '', '', '', '', '', 'C', '=', 'D', '>', 'A', '=', 'B', 'A', '=', 'B', '>', 'C', '=', 'D', '', '']\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', 'C2H5OH', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
            "[0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', 'O2', '', 'CO2', '', '']\n",
            "[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
            "[1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
            "[0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0]\n",
            "[0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0]\n",
            "===============================\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# training predict score\n",
        "total_result_list = []\n",
        "for q in range(len(training_data)):\n",
        "    precheck_sent = prepare_sequence(training_data[q][0], word_to_ix)\n",
        "    predict = model(precheck_sent)\n",
        "    answer = training_data[q][1]\n",
        "    print(training_data[q][0])\n",
        "    print(predict)\n",
        "    print(answer)\n",
        "    print(\"===============================\")\n",
        "    accuracy = accuracy_score(answer, predict)\n",
        "    precision = precision_score(answer, predict)\n",
        "    recall = recall_score(answer, predict)\n",
        "    f1score = f1_score(answer, predict)\n",
        "    result_list = [accuracy, precision, recall, f1score]\n",
        "    result_list = [round(elem, 2) for elem in result_list]\n",
        "    total_result_list.append(result_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.92</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.89</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.81</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.94</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.85</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.85</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.87</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.90</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.91</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.90</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.95</td>\n",
              "      <td>0.92</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.87</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.88</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.85</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.82</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.96</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.93</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.76</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.88</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.82</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.82</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    accuracy  precision  recall  f1score\n",
              "0       0.92       0.93    0.93     0.93\n",
              "1       0.80       0.91    0.77     0.83\n",
              "2       0.89       0.86    0.67     0.75\n",
              "3       0.81       0.88    0.78     0.82\n",
              "4       0.94       0.94    0.94     0.94\n",
              "5       0.85       1.00    0.60     0.75\n",
              "6       0.85       0.86    0.75     0.80\n",
              "7       0.87       1.00    0.64     0.78\n",
              "8       0.90       0.88    0.88     0.88\n",
              "9       0.91       0.89    0.89     0.89\n",
              "10      1.00       1.00    1.00     1.00\n",
              "11      0.90       0.86    0.92     0.89\n",
              "12      0.95       0.92    1.00     0.96\n",
              "13      0.87       0.92    0.86     0.89\n",
              "14      0.88       0.87    0.87     0.87\n",
              "15      0.85       1.00    0.64     0.78\n",
              "16      0.82       0.57    0.67     0.62\n",
              "17      0.96       1.00    0.93     0.96\n",
              "18      0.93       0.80    0.80     0.80\n",
              "19      0.76       0.62    0.71     0.67\n",
              "20      0.88       0.91    0.91     0.91\n",
              "21      1.00       1.00    1.00     1.00\n",
              "22      0.82       0.82    0.82     0.82"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_result_df = pd.DataFrame(total_result_list)\n",
        "total_result_df.columns = ['accuracy', 'precision', 'recall', 'f1score']\n",
        "total_result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "accuracy     0.885217\n",
              "precision    0.888696\n",
              "recall       0.825217\n",
              "f1score      0.849565\n",
              "dtype: float64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_result_df.mean(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "testing data show"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for testing data\n",
        "test_word_to_ix = {}\n",
        "for sentence, tags in testing_data:\n",
        "    for word in sentence:\n",
        "        if word not in test_word_to_ix:\n",
        "            test_word_to_ix[word] = len(test_word_to_ix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['', '', '', '', '', '1033.6', 'cm', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
            "[0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1]\n",
            "[0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0]\n",
            "===============================\n",
            "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'A', 'B', 'C', 'D', '', '', '', '', '', '', '', 'C', 'D', '', '', '', '', '', 'A', 'B', '', '', 'A', '=', 'B', '=', 'C', '=', 'D']\n",
            "[0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "===============================\n"
          ]
        }
      ],
      "source": [
        "total_result_list = []\n",
        "for q in range(len(testing_data)):\n",
        "    precheck_sent = prepare_sequence(testing_data[q][0], test_word_to_ix)\n",
        "    predict = model(precheck_sent)\n",
        "    answer = testing_data[q][1]\n",
        "    print(testing_data[q][0])\n",
        "    print(predict)\n",
        "    print(answer)\n",
        "    print(\"===============================\")\n",
        "    accuracy = accuracy_score(answer, predict)\n",
        "    precision = precision_score(answer, predict)\n",
        "    recall = recall_score(answer, predict)\n",
        "    f1score = f1_score(answer, predict)\n",
        "    result_list = [accuracy, precision, recall, f1score]\n",
        "    result_list = [round(elem, 2) for elem in result_list]\n",
        "    total_result_list.append(result_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.44</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.42</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.42</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.52</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  precision  recall  f1score\n",
              "0      0.44       0.00    0.00     0.00\n",
              "1      0.42       0.33    0.57     0.42\n",
              "2      0.42       0.31    0.62     0.42\n",
              "3      0.52       0.08    0.09     0.09"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_result_df = pd.DataFrame(total_result_list)\n",
        "total_result_df.columns = ['accuracy', 'precision', 'recall', 'f1score']\n",
        "total_result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "accuracy     0.4500\n",
              "precision    0.1800\n",
              "recall       0.3200\n",
              "f1score      0.2325\n",
              "dtype: float64"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_result_df.mean(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # draw confusion matrix\n",
        "# import matplotlib.pyplot as plt\n",
        "# conf_matrix = confusion_matrix(y_true=answer, y_pred=predict)\n",
        "# fig, ax = plt.subplots(figsize=(5, 5))\n",
        "# ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n",
        "# for i in range(conf_matrix.shape[0]):\n",
        "#     for j in range(conf_matrix.shape[1]):\n",
        "#         ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
        "\n",
        "# plt.xlabel('Predictions', fontsize=18)\n",
        "# plt.ylabel('Actuals', fontsize=18)\n",
        "# plt.title('Confusion Matrix', fontsize=18)\n",
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deepeye_env",
      "language": "python",
      "name": "deepeye_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
